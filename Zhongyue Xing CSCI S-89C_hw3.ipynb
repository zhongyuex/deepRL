{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Zhongyue Xing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Environment below has three states: 1, 2, and 3. Possible transitions are: (1) 1->1, 1->2; (2) 2->1, 2->2, 2->3; and (3) 3->2, 3->3.\n",
    "\n",
    "Actions of the Agent are decoded by -1, 0, and +1, which correspond to its intention to move left, stay, and move right, respectively. The Environment, however, does not always respond to these intentions exactly, and there is 10% chance that action 0 will result in moving to the left (if moving to the left is admissible), and 1 action will result in staying - in other words, there is an \"east wind\" (please see get_reward() of the Environment). \n",
    "\n",
    "Further, we assume that the number of steps, T, is infinite and whenever the process enters state 3, the Environment generates reward = 1. In all other cases the reward is 0. For example, transition 2->3 will result in reward 1, transition 3->3 will result in reward 1, transition 3->2 will result in reward 0, transition 2->2 will result in reward 0, etc.\n",
    "\n",
    "Let’s use $\\gamma=0.9$. Currently, the Agent always selects action 0 (has an intention to stay). Please notice that without the wind, the state-values would be [0,0,1/0.1] for this policy. With the wind, the value of state 3, however, becomes only 4.74 because the transition 3->2 happens sooner or later with probability 1. The Agent does not make an attempt to return to state 3 – its intention is to stay.\n",
    "\n",
    "Function reward_cumulative(T=10, S0=1, gamma=1) returns the observed cumulative discounted reward for T number of steps if the process starts in state S0. Please notice that given $\\gamma=0.9$, T=100 does not make this estimate much different from infinite time because $\\gamma^T$ is of order $10^{-5}$.\n",
    "\n",
    "Function V_estimate(T=10, S0=1, gamma=0.9, n_trials=10) calls reward_cumulative() function n_trials number of times and then estimates the state-value based on these n_trials paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 3: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if self.state > 1 and move > -1:\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        #actions = env.admissible_actions()\n",
    "        action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "        \n",
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time <= T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[0.   0.   4.78]\n"
     ]
    }
   ],
   "source": [
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,4)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problem 1 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please add state 4 and 5 to the Environment. For entering states 4 and 5 assume no rewards. Please keep current reward from entering state 3 as is, i.e. 2->3, 3->3, 4->3 will result in reward=1. All other cases correspond to 0 reward.\n",
    "\n",
    "For current actions of the Agent, keep $\\gamma=0.9$ and estimate the state-values using V_estimate() function with T = 100 and n_trials=10000. Print the result for states 1, 2, 3, 4, 5. What value of state 4 do you observe?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[0.   0.   4.76 2.78 1.31]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        move = action\n",
    "        if self.state > 1 and move > -1:\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        #actions = env.admissible_actions()\n",
    "        action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "        \n",
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time <= T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate\n",
    "\n",
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,6)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Environment you developed in Problem 1, change the actions of the Agent to the optimal, that is, from states 1 and 2 it will want to move to right, stay in stay 3, and move to left from states 4 and 5.\n",
    "\n",
    "For these actions of the Agent and $\\gamma=0.9$, estimate the state-values using V_estimate() function with T = 100 and n_trials=10000. Print the result for states 1, 2, 3, 4, 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[8.01 9.   9.   9.11 8.19]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt \n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self):\n",
    "        A = list((-1,0,1))\n",
    "        if self.state == 1: A.remove(-1)\n",
    "        if self.state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def check_state(self):\n",
    "        return self.state\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        self.time += 1\n",
    "        assert action in self.admissible_actions()\n",
    "        move = action\n",
    "        if self.state > 1 and move > -1 or self.state == 1 and action == 1:\n",
    "            move = np.random.choice([move-1, move],p=[0.1,0.9])\n",
    "        self.state += move\n",
    "        if self.state == 3:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "        return reward\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.current_reward = 0.0\n",
    "\n",
    "    def step(self, env):\n",
    "        actions = env.admissible_actions()\n",
    "        if env.state < 3:\n",
    "            action_selected = 1\n",
    "        elif env.state > 3:\n",
    "            action_selected = -1\n",
    "        else:\n",
    "            action_selected = 0\n",
    "        reward = env.get_reward(action_selected)            \n",
    "        self.current_reward = reward\n",
    "        \n",
    "def reward_cumulative(T=10, S0=1, gamma=1):\n",
    "    env = Environment(S0)\n",
    "    agent = Agent()\n",
    "    G = 0\n",
    "    while env.time <= T:\n",
    "        agent.step(env)\n",
    "        G += gamma**(env.time-1)*agent.current_reward\n",
    "    return G\n",
    "\n",
    "def V_estimate(T=10, S0=1, gamma=0.9, n_trials=10):\n",
    "    V_estimate = 0\n",
    "    for i in range(1,n_trials+1):\n",
    "        #V_estimate = (V_estimate*(i-1) + reward_cumulative(T, S0, gamma))/i\n",
    "        V_estimate = V_estimate+(reward_cumulative(T, S0, gamma)-V_estimate)/i \n",
    "    return V_estimate\n",
    "\n",
    "\n",
    "T = 100\n",
    "V = np.array([V_estimate(T, S0 = s, gamma=0.9, n_trials=10000) for s in range(1,6)])\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(\"state-value function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (15 points)\n",
    "For the Environment and Agent you developed in Problem 2, please obtain the state-value for this policy in an alternative way without simulations by solving the Bellman equation (eq. 4.5) numerically: use the iterative policy evaluation algorithm on p.75 of \"Reinforcement Learning\" by Sutton and Barto.\n",
    "\n",
    "Please notice that for these actions the policy $\\pi(a|s)$ is<br>\n",
    "$\\pi(-1|1)=0, \\pi(0|1)=0, \\pi(+1|1)=1$,<br>\n",
    "$\\pi(-1|2)=0, \\pi(0|2)=0, \\pi(+1|2)=1$,<br>\n",
    "$\\pi(-1|3)=0, \\pi(0|3)=1, \\pi(+1|3)=0$,<br>\n",
    "$\\pi(-1|4)=1, \\pi(0|4)=0, \\pi(+1|4)=0$,<br>\n",
    "etc.\n",
    "\n",
    "\n",
    "The non-zero transition probabilities $p(s^\\prime,r|s,a)$ are<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=1,a=0)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=1,a=+1)=0.1,p(s^\\prime=2,r=0|s=1,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=1,r=0|s=2,a=-1)=1$,<br>\n",
    "$p(s^\\prime=1,r=0|s=2,a=0)=0.1,p(s^\\prime=2,r=0|s=2,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=2,r=0|s=2,a=+1)=0.1,p(s^\\prime=3,r=1|s=2,a=+1)=0.9$,<br>\n",
    "\n",
    "$p(s^\\prime=2,r=0|s=3,a=-1)=1$,<br>\n",
    "$p(s^\\prime=2,r=0|s=3,a=0)=0.1,p(s^\\prime=3,r=1|s=3,a=0)=0.9$,<br>\n",
    "$p(s^\\prime=3,r=1|s=3,a=+1)=0.1,p(s^\\prime=4,r=0|s=3,a=+1)=0.9$,<br>\n",
    "\n",
    "etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state-value function:\n",
      "[8.01, 9.0, 9.0, 9.1, 8.19]\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self, S0 = 1):\n",
    "        self.time = 0\n",
    "        self.state = S0\n",
    "\n",
    "    def admissible_actions(self, state):\n",
    "        \"\"\"returns possible actions of a given state\"\"\"\n",
    "        A = list((-1,0,1))\n",
    "        if state == 1: A.remove(-1)\n",
    "        if state == 5: A.remove(1)\n",
    "        return A\n",
    "    \n",
    "    def possible_outcomes(self, state, action):\n",
    "        \"\"\" \n",
    "        returns list of possible (outcome states, reward pairs) after\n",
    "        conducting an action\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        assert action in self.admissible_actions(state), \"action {} at state {}!\".format(action, state)\n",
    "        if state > 1 and action > -1 or state == 1 and action == 1:\n",
    "            outcome_states = [state+action, state+action-1]\n",
    "        else:\n",
    "            outcome_states = [state+action]\n",
    "\n",
    "        outcome_pair = []\n",
    "        for outcome_state in outcome_states:\n",
    "            if outcome_state == 3:\n",
    "                outcome_pair.append((outcome_state, 1))\n",
    "            else:\n",
    "                outcome_pair.append((outcome_state, 0))\n",
    "        return outcome_pair\n",
    "\n",
    "\n",
    "    def transition_prob(self, new_state, reward, cur_state, action):\n",
    "        \"\"\"\n",
    "        returns transition probabilities for given:\n",
    "            new_state (s'), reward (r), current state (s), and action (a)\n",
    "        \"\"\"\n",
    "        # valid actions\n",
    "        assert action in self.admissible_actions(cur_state)\n",
    "\n",
    "        # valid reward\n",
    "        if (reward == 1 and new_state == 3) or (reward == 0 and new_state != 3):\n",
    "            # valid new state with no wind effect\n",
    "            if new_state == cur_state + action:\n",
    "                if action == -1 or (action == 0 and cur_state == 1):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0.9\n",
    "            # valid new state with wind effect\n",
    "            elif new_state == cur_state + (action - 1):\n",
    "                return 0.1\n",
    "\n",
    "\n",
    "        # remaining combinations are invalid\n",
    "        return 0\n",
    "\n",
    "    def all_states(self):\n",
    "        return list(range(1, 6))\n",
    "\n",
    "\n",
    "\n",
    "def policy(action, state):\n",
    "    if (state < 3 and action == 1) or (\n",
    "        state > 3 and action == -1) or(\n",
    "        state == 3 and action == 0):\n",
    "        return 1\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def iterative_policy_eval(env, pi, gamma = 0.9, theta = 0.01):\n",
    "    \"\"\"\n",
    "    Input policy a pi function and information of the environment\n",
    "    to estimate its expected value under pi. Return a default dictionary\n",
    "    of state:expected_values\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # start with all 0 value function\n",
    "    V = defaultdict(float)\n",
    "\n",
    "    counter = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in env.all_states():\n",
    "            v = V[state]\n",
    "            sumV = 0\n",
    "\n",
    "            # summation (refer to textbook pg. 75)\n",
    "            for action in env.admissible_actions(state):\n",
    "                action_prob = pi(action, state)\n",
    "                for new_state, reward in env.possible_outcomes(state, action):\n",
    "                    transition_prob = env.transition_prob(new_state, reward, state, action)\n",
    "                    #print(state, action, new_state, reward, action_prob, transition_prob)\n",
    "                    sumV += action_prob * transition_prob *(reward + gamma*V[new_state])\n",
    "            V[state] = sumV\n",
    "\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "print(\"state-value function:\")\n",
    "V = iterative_policy_eval(\n",
    "    env = Environment(), \n",
    "    pi = lambda action, state: policy(action, state),\n",
    "    gamma = 0.9, theta = 0.0000000000000000000000001\n",
    ")\n",
    "print([round(value,2) for _, value in sorted([state_value_pair for state_value_pair in V.items()])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
