{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Name: Zhongyue Xing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antiviral drugs are used for treating viral infections. The procedure often requires a sequence of treatments. Over time patients develop resistance to the drugs and need to keep switching them. Moreover, a drug of type 1 can make the patient (partially) resistant to the drug of type 2.\n",
    "\n",
    "In this problem we consider drugs of type A, B, and C, all of which can potentially lower the viral load but the effect depends on their order. The 'Viral_load.xlsx' dataset provides results for 1,000 of patients, where rewards (based on the observed viral load level of patients) indicate the effectiveness of the preceding drugs used. For example, the first patient used drugs A, B, and C in this order and received the following rewards: 14.9062033, 13.67860579, and 12.76096513, respectively.\n",
    "\n",
    "Because of the antiviral drug resistance, same drug cannot be used twice. The policy (behavior policy $b$) that was used for treatments represented in the 'Viral_load.xlsx' dataset is\n",
    "\n",
    "1) if all drugs are available, pick A with probability 0.7, B with probability 0.2, and C with probability 0.1   \n",
    "2) if all drugs, but A, are available, pick B with probability 0.65 and C with probability 0.35   \n",
    "3) if all drugs, but B, are available, pick A with probability 0.9 and C with probability 0.1  \n",
    "4) if all drugs, but C, are available, pick A with probability 0.8 and B with probability 0.2   \n",
    "5) if only one drug is available, pick it with probability 1    \n",
    "\n",
    "\n",
    "\n",
    "One can find that based on these observations, the optimal order of the treatment is B, A, and then C, indicating that drug A diminishes the effect of drug B, and C diminishes the effect of drug A (and possibly B).\n",
    "\n",
    "The sequence of treatments can be represented as a Markov Decision Process (MDP). Clearly, the set of available actions at time t depends on the intire history. In order to model the treatment proceedure with the MDP, we introduce states as follows:\n",
    "\n",
    "1) $S_{ABC}=${all drugs are available}   \n",
    "2) $S_{BC}=${all drugs, except A, are available}   \n",
    "3) $S_{AC}=${all drugs, except B, are available}       \n",
    "4) $S_{AB}=${all drugs, except C, are available}   \n",
    "5) $S_{A}=${only A is available}   \n",
    "6) $S_{B}=${only B is available}   \n",
    "7) $S_{C}=${only C is available}      \n",
    "\n",
    "The optimal policy $\\pi$, that corresponds to the sequence B, A, C, is then: (1) select B in state $S_{ABC}$; (2) select A in state $S_{AC}$; (3) select C in state $S_{C}$. The other actions do not need to be specified because the MDP will never reach them.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem, create a function that returns the importance-sampling ratio for each sequence of type \"ABC\" (use input format of choice), given target and behavior policies, $\\pi$ and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.555555555555555"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = {\n",
    "    'ABC': {'A': 0.7, 'B': 0.2, 'C': 0.1},\n",
    "    'BC': {'B': 0.65, 'C': 0.35},\n",
    "    'AC': {'A': 0.9, 'C': 0.1},\n",
    "    'AB': {'A': 0.8, 'B': 0.2},\n",
    "    'A': {'A': 1},\n",
    "    'B': {'B': 1},\n",
    "    'C': {'C': 1}\n",
    "    }\n",
    "\n",
    "pi = {\"ABC\": 'B',\n",
    "     \"AC\": 'A',\n",
    "     \"C\": 'C'}\n",
    "\n",
    "def rho(S, A, b = b, pi = pi):\n",
    "    \"\"\"\n",
    "    Returns the importance sampling ratio for optimistic policy pi and behavioral policy b given\n",
    "    a sequence of states and actions in lists.\n",
    "    S = [S_t, S_(t+1), ...., S_(T-1), S_T]\n",
    "    A = [A_t, A_(t+1), ...., A_(T-1), A_T]\n",
    "    Pi should be an admissible, non-empty, and deterministic policy.\n",
    "    \n",
    "    \"\"\"\n",
    "    assert len(S) == len(A)\n",
    "\n",
    "    r = 1\n",
    "    for k in range(len(S)):\n",
    "        if S[k] in pi.keys():\n",
    "            if A[k] == pi[S[k]]:\n",
    "                r *= 1/b[S[k]][A[k]]\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    return r\n",
    "\n",
    "rho([\"ABC\", \"AC\", \"C\"], [\"B\", \"A\", \"C\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 'Viral_load.xlsx' dataset, estimate the state value function $v_\\pi(s)$ for $s\\in\\{s_{ABC},s_{AC},s_{C}\\}$ via Off-policy MC prediction (section 5.6 of \"Reinforcement Learning\" by Sutton and Barto). Please notice that generating an episode under policy $b$ would correspond to reading a row from 'Viral_load.xlsx'. Print the result for $\\gamma=0.99$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding state value following optimal policy are:\n",
      "{'ABC': 41.507588616523094, 'AC': 27.8264130004963, 'C': 14.025463818322676}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import copy\n",
    "\n",
    "def MC_state_value_estimate(gamma = 0.99):\n",
    "    V = {state:0 for state in [\"ABC\", \"AC\", \"C\"]}\n",
    "    C = copy.copy(V)\n",
    "    \n",
    "    csv_file = open(\"Viral_load.csv\") # file closed upon return statement\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader) # skip header row\n",
    "\n",
    "    while True:\n",
    "        # Get an episode\n",
    "        try:\n",
    "            episode = next(reader)[1:]\n",
    "            states = [\"ABC\"]\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            for t in range(len(states[0])):\n",
    "                actions.append(episode[t*2])\n",
    "                rewards.append(float(episode[t*2+1]))\n",
    "                new_state = states[t].replace(actions[t], '')\n",
    "                if new_state:\n",
    "                    states.append(new_state)\n",
    "\n",
    "        # reached final episode\n",
    "        except StopIteration:\n",
    "            csv_file.close()\n",
    "            return V\n",
    "\n",
    "        G = 0\n",
    "        W = 1\n",
    "        for t in range(-1, -len(states)-1, -1):\n",
    "            if W == 0:\n",
    "                break\n",
    "\n",
    "            # get state, action and reward at t\n",
    "            s = states[t]\n",
    "            if s not in V.keys():\n",
    "                break\n",
    "            a = actions[t]\n",
    "            r = rewards[t]\n",
    "\n",
    "            # update relevant values\n",
    "            G = gamma*G + r\n",
    "            C[s] += W\n",
    "            V[s] += W/C[s] * (G-V[s])\n",
    "            W *= rho([s], [a])\n",
    "\n",
    "\n",
    "print(\"The corresponding state value following optimal policy are:\")\n",
    "print(MC_state_value_estimate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (10 points)\n",
    "Modify the algorithm you developed in Problem 2 in order to estimate $v_\\pi(s)$ for $s\\in\\{s_{ABC},s_{AC},s_{C}\\}$ via Off-policy one-step TD prediction. Print the result for $\\gamma=0.99$ and $\\alpha=0.15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corresponding state value following optimal policy are:\n",
      "{'ABC': 40.89128819163807, 'AC': 27.668808615308315, 'C': 14.073557148762944}\n"
     ]
    }
   ],
   "source": [
    "def TD_state_value_estimate(gamma = 0.99, alpha = 0.15):\n",
    "    V = {state:0 for state in [\"ABC\", \"AC\", \"C\",'']}\n",
    "    \n",
    "    csv_file = open(\"Viral_load.csv\") # file closed upon return statement\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader) # skip header row\n",
    "\n",
    "    while True:\n",
    "        # Get an episode\n",
    "        try:\n",
    "            episode = next(reader)[1:]\n",
    "\n",
    "        # terminate after final episode\n",
    "        except StopIteration:\n",
    "            csv_file.close()\n",
    "            del V[''] # remove terminal state place-holder\n",
    "            return V\n",
    "\n",
    "        G = 0\n",
    "        s = \"ABC\"\n",
    "        for t in range(len(episode)//2):\n",
    "            a = episode[t*2]\n",
    "            if a != pi[s]:\n",
    "                break\n",
    "            s_new = s.replace(a, '')\n",
    "            r = float(episode[t*2+1])\n",
    "\n",
    "            V[s] += alpha*rho([s], [a])*(r + gamma*V[s_new] - V[s])\n",
    "\n",
    "            s = s_new\n",
    "\n",
    "\n",
    "print(\"The corresponding state value following optimal policy are:\")\n",
    "print(TD_state_value_estimate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
